# ChatGPT_like_Transformer
# WIDS 5.0 â€“ Coding a GPT-like Transformer from Scratch
-During This project I learnt from basics I went from Neural Networks basics ,Deep Learning Foundations
By Watching recommended videos and stanford courses, Then learnt "PyTorch" , then continued with Understanding -Attention Mechanism
-Grasped **how decoder-only transformers work**
-Then Built The Bigram Language Model , This is a simple and dumb language model
-WHAT THIS MODEL DOES is predicts the next character based only on the current character , it has no memory of the earlier context , Thus generates text that looks broken / gibberish 

